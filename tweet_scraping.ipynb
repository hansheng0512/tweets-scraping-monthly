{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/JustAnotherArchivist/snscrape.git\n",
      "  Cloning https://github.com/JustAnotherArchivist/snscrape.git to c:\\users\\annabelle\\appdata\\local\\temp\\pip-req-build-ulp3_dor\n",
      "  Resolved https://github.com/JustAnotherArchivist/snscrape.git to commit a6b6f3faaa26f541d9469651451340096b5abc92\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\annabelle\\anaconda3\\envs\\tweet-scrape\\lib\\site-packages (from snscrape==0.3.5.dev138+ga6b6f3f) (2.26.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\annabelle\\anaconda3\\envs\\tweet-scrape\\lib\\site-packages (from snscrape==0.3.5.dev138+ga6b6f3f) (4.6.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\annabelle\\anaconda3\\envs\\tweet-scrape\\lib\\site-packages (from snscrape==0.3.5.dev138+ga6b6f3f) (4.10.0)\n",
      "Requirement already satisfied: pytz in c:\\users\\annabelle\\anaconda3\\envs\\tweet-scrape\\lib\\site-packages (from snscrape==0.3.5.dev138+ga6b6f3f) (2021.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\annabelle\\anaconda3\\envs\\tweet-scrape\\lib\\site-packages (from beautifulsoup4->snscrape==0.3.5.dev138+ga6b6f3f) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\annabelle\\appdata\\roaming\\python\\python38\\site-packages (from requests[socks]->snscrape==0.3.5.dev138+ga6b6f3f) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\annabelle\\anaconda3\\envs\\tweet-scrape\\lib\\site-packages (from requests[socks]->snscrape==0.3.5.dev138+ga6b6f3f) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\annabelle\\anaconda3\\envs\\tweet-scrape\\lib\\site-packages (from requests[socks]->snscrape==0.3.5.dev138+ga6b6f3f) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\annabelle\\appdata\\roaming\\python\\python38\\site-packages (from requests[socks]->snscrape==0.3.5.dev138+ga6b6f3f) (1.26.6)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\annabelle\\anaconda3\\envs\\tweet-scrape\\lib\\site-packages (from requests[socks]->snscrape==0.3.5.dev138+ga6b6f3f) (1.7.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/JustAnotherArchivist/snscrape.git 'C:\\Users\\annabelle\\AppData\\Local\\Temp\\pip-req-build-ulp3_dor'\n"
     ]
    }
   ],
   "source": [
    "! pip3 install git+https://github.com/JustAnotherArchivist/snscrape.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Scraping Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argument of snscraper using `text-query-tweets`: \n",
    "```\n",
    "'telegram-channel', 'vkontakte-user', 'weibo-user', 'facebook-group', 'instagram-user', 'instagram-hashtag', 'instagram-location', 'reddit-user', 'reddit-subreddit', 'reddit-search', 'twitter-search', 'twitter-tweet', 'facebook-user', 'facebook-community', 'twitter-user', 'twitter-hashtag', 'twitter-list-posts', 'twitter-profile' \n",
    "```\n",
    "\n",
    "For example, we can use \"twitter-hashtag\" to scrape tweets based the hashtags. \n",
    "The command below scrape tweets with hashtags \"yoga\" or \"exercise\" between the date \"2021-03-01\" and \"2021-05-20\" capped at maximum 2000 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg = \"twitter-hashtag\"\n",
    "os.system(f'snscrape --jsonl --progress --max-results 2000 --since 2020-03-01 {arg} \"yoga exercise until:2021-05-20\" > text-query-tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert JSON file to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the json generated from the cell above and create a pandas dataframe\n",
    "tweets_df = pd.read_json('text-query-tweets.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_type', 'url', 'date', 'content', 'renderedContent', 'id', 'user',\n",
       "       'replyCount', 'retweetCount', 'likeCount', 'quoteCount',\n",
       "       'conversationId', 'lang', 'source', 'sourceUrl', 'sourceLabel',\n",
       "       'outlinks', 'tcooutlinks', 'media', 'retweetedTweet', 'quotedTweet',\n",
       "       'inReplyToTweetId', 'inReplyToUser', 'mentionedUsers', 'coordinates',\n",
       "       'place', 'hashtags', 'cashtags'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tweets_df[[\"id\", \"url\", \"date\", \"content\",\n",
    "                \"hashtags\", \"cashtags\", \"media\", \"lang\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tweet_data_single.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulk Scraping Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below run the tweets scraping for different hashtag and time intervals and save them in a Dataframe, at last to a CSV file. \n",
    "\n",
    "For example, we provide these hashtags, \"valentines\", \"christmas\", \"halloween\". The command below scrape tweets with the hashtags provided between the date \"2021-08-01\" and \"2021-11-01\" capped at maximum 100 tweets each month for each hashtag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag = [\"valentines\", \"christmas\", \"halloween\"] #hashtag to scrape\n",
    "date_interval = [\"2021-08-01\", \"2021-09-01\", \"2021-10-01\", \"2021-11-01\"] #to ensure there are tweets from different month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valentines (2021-08-01->2021-09-01): 100 \n",
      "christmas (2021-08-01->2021-09-01): 100 \n",
      "halloween (2021-08-01->2021-09-01): 100 \n",
      "valentines (2021-09-01->2021-10-01): 100 \n",
      "christmas (2021-09-01->2021-10-01): 100 \n",
      "halloween (2021-09-01->2021-10-01): 100 \n",
      "valentines (2021-10-01->2021-11-01): 100 \n",
      "christmas (2021-10-01->2021-11-01): 100 \n",
      "halloween (2021-10-01->2021-11-01): 100 \n"
     ]
    }
   ],
   "source": [
    "data = None\n",
    "for i in range(len(date_interval)-1):\n",
    "    for hash in hashtag:\n",
    "        os.system(\n",
    "            f'snscrape --jsonl --progress --max-results 100 --since {date_interval[i]} {arg} \"{hash} until:{date_interval[i+1]}\" > text-query-tweets.json')\n",
    "        tweets_df = pd.read_json('text-query-tweets.json', lines=True)\n",
    "        df = tweets_df[[\"id\", \"url\", \"date\", \"content\",\n",
    "                        \"hashtags\", \"cashtags\", \"media\", \"lang\"]]\n",
    "        if data is None:\n",
    "            data = df\n",
    "        else:\n",
    "            data = data.append(df)\n",
    "        print(f\"{hash} ({date_interval[i]}->{date_interval[i+1]}): {len(df)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('tweet_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "tweet_hashtag_scrapping.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
